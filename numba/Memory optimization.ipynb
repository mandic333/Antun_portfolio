{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the memory performance of the GPU\n",
    "\n",
    "\n",
    "In this section we are going to investigate a crucial aspect of the memory locality on the GPUs. It should be perceived in a slightly different way than on the CPUs. To demonstrate this, we will use BLAS matrix-vector kernel. \n",
    "\n",
    "The threads of the GPU operate row-wise in the input matrix, each one taking care of a single row to compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CUDA transfer overheads: 0.010683199688792229\n",
      "  CUDA kernel time: 0.0015128639936447144\n",
      "  Consumed memory bandwidth: 5.555682490500104 GB/s\n",
      "Total time (GPU): 0.015623807907104492 s\n",
      "Total time (CPU): 0.0065288543701171875 s\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "class time_region:\n",
    "    def __init__(self, time_offset=0):\n",
    "        self._time_off = time_offset\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._t_start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self._t_end = time.time()\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        return self._time_off + (self._t_end - self._t_start)\n",
    "\n",
    "\n",
    "class time_region_cuda:\n",
    "    def __init__(self, time_offset=0, cuda_stream=0):\n",
    "        self._t_start = cuda.event(timing=True)\n",
    "        self._t_end = cuda.event(timing=True)\n",
    "        self._time_off = time_offset\n",
    "        self._cuda_stream = cuda_stream\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._t_start.record(self._cuda_stream)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self._t_end.record(self._cuda_stream)\n",
    "        self._t_end.synchronize()\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        return self._time_off + 1.e-3*cuda.event_elapsed_time(self._t_start,\n",
    "                                                              self._t_end)\n",
    "\n",
    "\n",
    "@cuda.jit('void(float64, Array(float64, 2, \"C\"), Array(float64, 1, \"C\"), '\n",
    "          'float64, Array(float64, 1, \"C\"))')\n",
    "def _gemv_cuda(alpha, A, x, beta, y):\n",
    "    i = cuda.grid(1)\n",
    "    N, M = A.shape\n",
    "    if i >= N:\n",
    "        return\n",
    "\n",
    "    prod = 0.0\n",
    "    for j in range(M):\n",
    "        prod += A[i, j]*x[j]\n",
    "\n",
    "    y[i] = alpha*prod + beta*y[i]\n",
    "\n",
    "\n",
    "def gemv_gpu(alpha, A, x, beta, y):\n",
    "    # Works only for square matrices\n",
    "    N = A.shape[0]\n",
    "    with time_region_cuda() as t_xfer:\n",
    "        d_A = cuda.to_device(A)\n",
    "        d_x = cuda.to_device(x)\n",
    "        d_y = cuda.to_device(y)\n",
    "        y_ret = cuda.pinned_array(N)\n",
    "        \n",
    "    block_size = 128\n",
    "    num_blocks = N // block_size\n",
    "    if N % block_size:\n",
    "        num_blocks += 1\n",
    "\n",
    "    with time_region_cuda() as t_kernel:\n",
    "        _gemv_cuda[num_blocks, block_size](alpha, d_A, d_x, beta, d_y)\n",
    "\n",
    "    with time_region_cuda(t_xfer.elapsed_time()) as t_xfer:\n",
    "        d_y.copy_to_host(y_ret)\n",
    "\n",
    "    print(f'  CUDA transfer overheads: {t_xfer.elapsed_time()}')\n",
    "    print(f'  CUDA kernel time: {t_kernel.elapsed_time()}')\n",
    "    print(f'  Consumed memory bandwidth: {1e-9*8*N*(N+2)/t_kernel.elapsed_time()} GB/s')\n",
    "    return y_ret\n",
    "\n",
    "N = 1024\n",
    "A = np.random.rand(N, N)\n",
    "x = np.random.rand(N)\n",
    "y_orig = np.ones(N)\n",
    "alpha = 0.2\n",
    "beta = 1\n",
    "\n",
    "with time_region() as t_gpu:\n",
    "    y = gemv_gpu(alpha, A, x, beta, y_orig)\n",
    "\n",
    "with time_region() as t_ref:\n",
    "    y_ref = alpha*(A @ x) + beta*y_orig\n",
    "    \n",
    "    \n",
    "cuda.profile_stop()\n",
    "\n",
    "print(f'Total time (GPU): {t_gpu.elapsed_time()} s')\n",
    "print(f'Total time (CPU): {t_ref.elapsed_time()} s')\n",
    "\n",
    "assert np.allclose(y, y_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU memory bandwidth consumed: 1.2873609248308502\n"
     ]
    }
   ],
   "source": [
    "print(f'CPU memory bandwidth consumed: {1e-9*8*N*(N+2)/t_ref.elapsed_time()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is going on? \n",
    "\n",
    "The matrix is stored in the default, row-major, order and each thread is assigned a row of the matrix.\n",
    "\n",
    "Assigning a column to each thread would require a reduction operations across all the threads on the device. Global reduction on GPUs is not straightforward to achieve, because there can't be any sort of global synchronization. We would have to write multiple kernels to achieve a global reduction. For reduction operations, we should use the `@cuda.reduce` decorator (see [here](http://numba.pydata.org/numba-doc/latest/cuda/reduction.html) for more information).\n",
    "\n",
    "In this arrangement subsequent threads access non-contiguous memory, but each thread accesses memory sequentially. This would be almost ideal on the CPU: perfect read access on both the $A$ matrix and the $x$ vector. What is \"wrong\" with the GPUs?\n",
    "\n",
    "For the CPUs the ideal is to assign a chunk of rows to each thread, because the above arrangement would lead to false sharing.\n",
    "\n",
    "The global memory on the GPU is organized in 256-byte memory segments and can be accessed in transactions of 32, 64 or 128 bytes. If all the threads of a warp access the same memory segments, a maximum of two memory transactions will be performed in order to fetch the values required by all the threads of the warp. This is called *memory coalescing* in CUDA's terminology. In our example, a warp needs to fetch $32\\times 8 = 256$ bytes, i.e., 2 memory transactions of 128 bytes each ideally. However, due to the row-major layout of the matrix $A$, each thread does a separate 32 byte transaction, generating $32\\times 32 = 1024$ bytes memory traffic per warp, which is four times more than the ideal! How much of the memory bandwidth did our code utilize?\n",
    "\n",
    "## Taking advantage of the shared memory\n",
    "\n",
    "On every GPU multiprocessor chip (SM), there is an on-chip memory called *shared memory*. This can either act as L1 data cache memory (default mode) or it can serve as programmable scratchpad at the disposal of the programmer. This memory is shared among all the threads of a CUDA block. We are going to modify our matrix-vector kernel to make use of it. If we inspect closer the algorithm, we will see that although there is no temporal locality in accesses to the matrix $A$, but the the vector $x$ is reused $N$ times. For this reason, we are going to cache the vector $x$ manually into the shared memory.\n",
    "\n",
    "We essentially process the matrix with a sliding window. The threads of a block (still 1D) undertake a double role: first, they fetch the $x$ vector elements into the shared memory of the block and, second, they perform the multiplication and the reduction to a local register variable. As soon as they process the full row, the store this value back to vector $y$.\n",
    "\n",
    "Here is the implementation in Numba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CUDA transfer overheads: 0.4772881617546082\n",
      "  CUDA kernel time: 0.03920246505737305\n",
      "  Consumed memory bandwidth: 13.698168806836362 GB/s\n",
      "Total time (GPU): 0.5861525535583496 s\n",
      "Total time (CPU): 0.05336952209472656 s\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "class time_region:\n",
    "    def __init__(self, time_offset=0):\n",
    "        self._time_off = time_offset\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._t_start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self._t_end = time.time()\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        return self._time_off + (self._t_end - self._t_start)\n",
    "\n",
    "\n",
    "class time_region_cuda:\n",
    "    def __init__(self, time_offset=0, cuda_stream=0):\n",
    "        self._t_start = cuda.event(timing=True)\n",
    "        self._t_end = cuda.event(timing=True)\n",
    "        self._time_off = time_offset\n",
    "        self._cuda_stream = cuda_stream\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._t_start.record(self._cuda_stream)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self._t_end.record(self._cuda_stream)\n",
    "        self._t_end.synchronize()\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        return self._time_off + 1.e-3*cuda.event_elapsed_time(self._t_start,\n",
    "                                                              self._t_end)\n",
    "BLOCK_SIZE = 128\n",
    "    \n",
    "@cuda.jit('void(float64, Array(float64, 2, \"F\"), Array(float64, 1, \"F\"), '\n",
    "          'float64, Array(float64, 1, \"F\"))')\n",
    "def _gemv_cuda_shared(alpha, A, x, beta, y):\n",
    "    i = cuda.grid(1)\n",
    "    N, M = A.shape\n",
    "    if i >= N:\n",
    "        return\n",
    "\n",
    "    lx = cuda.shared.array(shape=BLOCK_SIZE, dtype=numba.float64)\n",
    "    bsize = cuda.blockDim.x\n",
    "    tid = cuda.threadIdx.x\n",
    "    num_blocks = cuda.gridDim.x\n",
    "\n",
    "    prod = 0.0\n",
    "    for b in range(num_blocks):\n",
    "        lx[tid] = x[tid + b*bsize]\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        for j in range(BLOCK_SIZE):\n",
    "            prod += A[i, j + b*bsize]*lx[j]\n",
    "\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    y[i] = alpha*prod + beta*y[i]\n",
    "\n",
    "def gemv_gpu(alpha, A, x, beta, y):\n",
    "    # Works only for square matrices\n",
    "    N = A.shape[0]\n",
    "    with time_region_cuda() as t_xfer:\n",
    "        d_A = cuda.to_device(A)\n",
    "        d_x = cuda.to_device(x)\n",
    "        d_y = cuda.to_device(y)\n",
    "        y_ret = cuda.pinned_array(N)\n",
    "        \n",
    "    num_blocks = N // BLOCK_SIZE\n",
    "    if N % BLOCK_SIZE:\n",
    "        num_blocks += 1\n",
    "\n",
    "    with time_region_cuda() as t_kernel:\n",
    "        _gemv_cuda_shared[num_blocks, BLOCK_SIZE](alpha, d_A, d_x, beta, d_y)\n",
    "\n",
    "    with time_region_cuda(t_xfer.elapsed_time()) as t_xfer:\n",
    "        d_y.copy_to_host(y_ret)\n",
    "\n",
    "    print(f'  CUDA transfer overheads: {t_xfer.elapsed_time()}')\n",
    "    print(f'  CUDA kernel time: {t_kernel.elapsed_time()}')\n",
    "    print(f'  Consumed memory bandwidth: {1e-9*8*N*(N+2)/t_kernel.elapsed_time()} GB/s')\n",
    "    return y_ret\n",
    "\n",
    "N = 1024*8\n",
    "A = np.asarray(np.random.rand(N, N), order='F')\n",
    "x = np.random.rand(N)\n",
    "y_orig = np.ones(N)\n",
    "alpha = 0.2\n",
    "beta = 1\n",
    "\n",
    "with time_region() as t_gpu:\n",
    "    y = gemv_gpu(alpha, A, x, beta, y_orig)\n",
    "\n",
    "with time_region() as t_ref:\n",
    "    y_ref = alpha*(A @ x) + beta*y_orig\n",
    "    \n",
    "    \n",
    "cuda.profile_stop()\n",
    "\n",
    "print(f'Total time (GPU): {t_gpu.elapsed_time()} s')\n",
    "print(f'Total time (CPU): {t_ref.elapsed_time()} s')\n",
    "\n",
    "assert np.allclose(y, y_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel is more complex now, but let's take it step by step. The `cuda.shared.array(shape=BLOCK_SIZE, dtype=numba.float64)` statement allocates the part of the $x$ vector that is stored in shared memory. When allocating an array in shared memory, the shape must be \"constant.\" Although constants don't exist in Python, in the context of Numba, this means that at the time the CUDA kernel is compiled, its value must be known.\n",
    "\n",
    "> In pure CUDA, you may write a kernel with an unknown shared memory array by declaring it `extern` and passing its size at the invocation of the kernel. In Numba, you can't do that.\n",
    "\n",
    "The main work of the algorithm is done in the following loop:\n",
    "\n",
    "```python\n",
    "    prod = 0.0\n",
    "    for b in range(num_blocks):\n",
    "        lx[tid] = x[tid + b*bsize]\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        for j in range(BLOCK_SIZE):\n",
    "            prod += A[i, j + b*bsize]*lx[j]\n",
    "\n",
    "        cuda.syncthreads()\n",
    "        \n",
    "    y[i] = alpha*prod + beta*y[i]\n",
    "```\n",
    "\n",
    "The outer loop iterates over the blocks in the $j$ direction and each thread fetches an element from $x$ and places it in the shared memory buffer. Before proceeding to the actual computation, we need to make sure that the shared buffer has been populated fully, thus we insert a thread barrier with `cuda.syncthreads()`. This barrier affects *only* the threads of a single block. We then move to the actual computation where each thread computes a row inside the sliding window. At the end we have to synchronize again, since we need to make sure that the `lx` buffer is consumed fully, before we start refilling it in the next iteration. Finally, after the threads have gone through all the blocks in the $j$ direction, they compute the final result in $y$.\n",
    "\n",
    "This kernel is 20% slower than the seemingly naive one. In fact, we shouldn't have expected any improvement whatsoever, since the \"naive\" kernel already hits the memory bandwidth limit. But why manually caching didn't work? When the shared memory is not used as a scratchpad memory, it functions as a standard L1 cache. So, essentially, the caching of $x$ did happen implicitly in the original version. The only thing we have achieved with the shared memory version is to introduce an additional computational overhead due to the sliding window; `nvprof` shows that. Truth must be said though, that in the early generations of NVIDIA GPUs, where shared memory did not function as a standard cache, this optimization did give you additional performance.\n",
    "\n",
    "So when is shared memory beneficial? It is only when you have a kernel with a high arithmetic intensity, which implies some significant temporal locality in the memory accesses. The most prominent example of such a kernel is the matrix-matrix multiplication, which has an arithmetic intensity at the order of $N$. In such a case, you could use the shared memory to implement the tiling algorithm. An example implementation with Numba an be found [here](http://numba.pydata.org/numba-doc/latest/cuda/examples.html?highlight=matrix%20matrix#matrix-multiplication). Another candidate user of shared memory could be layered stencil computations, where you could cache the intermediate layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
